import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def load_data(file_path, target_column='PCE', log_transform=True):
    """
    ä»æŒ‡å®šè·¯å¾„åŠ è½½ CSV æ•°æ®ï¼Œè¿›è¡Œåˆæ­¥æ¸…æ´—ï¼Œå¹¶åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡ã€‚
    
    å‚æ•°:
        file_path (str): CSV æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        target_column (str): ç›®æ ‡å˜é‡çš„åˆ—åï¼Œé»˜è®¤ä¸º 'PCE'ã€‚
        log_transform (bool): æ˜¯å¦å¯¹ç›®æ ‡å˜é‡è¿›è¡Œ log1p å˜æ¢ã€‚
        
    è¿”å›:
        X (pd.DataFrame): ç‰¹å¾æ•°æ®ã€‚
        y (pd.Series): ç›®æ ‡å˜é‡æ•°æ®ã€‚
    """
    print(f"\nğŸš€ æ­£åœ¨ä» {file_path} è¯»å–æ•°æ®...")
    df = pd.read_csv(file_path)
    
    # å¦‚æœå­˜åœ¨ 'Unnamed: 0' åˆ—ï¼Œåˆ™åˆ é™¤å®ƒ
    if 'Unnamed: 0' in df.columns:
        df = df.drop(columns=['Unnamed: 0'])
        print("å·²åˆ é™¤ 'Unnamed: 0' åˆ—ã€‚")

    # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
    if target_column not in df.columns:
        raise ValueError(f"âŒ é”™è¯¯: ç›®æ ‡åˆ— '{target_column}' åœ¨æ•°æ®é›†ä¸­ä¸å­˜åœ¨ã€‚")
        
    # åˆ é™¤æ ‡ç­¾åˆ—ä¸ºç©ºçš„è¡Œ
    initial_rows = len(df)
    df = df.dropna(subset=[target_column])
    if len(df) < initial_rows:
        print(f"è­¦å‘Š: å·²åˆ é™¤ {initial_rows - len(df)} è¡Œï¼Œå› ä¸ºç›®æ ‡åˆ— '{target_column}' å­˜åœ¨ç¼ºå¤±å€¼ã€‚")

    # æå–ç›®æ ‡å˜é‡ y å’Œç‰¹å¾ X
    y = df.pop(target_column)
    X = df
    
    if log_transform:
        y = np.log1p(y)
        print(f"âœ… å·²å¯¹ç›®æ ‡å˜é‡ '{target_column}' è¿›è¡Œ log1p å˜æ¢ã€‚")
        
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆã€‚ç‰¹å¾å½¢çŠ¶: {X.shape}, ç›®æ ‡å½¢çŠ¶: {y.shape}")
    return X, y

def clean_data_manual(
    X,
    y,
    variance_threshold=0.0,
    missing_ratio_limit=0.3,
    correlation_threshold=0.95
    ):
    """
    å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œæ¸…æ´—ï¼ŒåŒ…æ‹¬å¤„ç†é”™è¯¯ç ã€ç¼ºå¤±å€¼è¿‡æ»¤ã€æ–¹å·®è¿‡æ»¤ã€ç›¸å…³æ€§è¿‡æ»¤å’Œæ ‡å‡†åŒ–ã€‚
    ä¿æŒç‰¹å¾ä¸æ ‡ç­¾çš„ç´¢å¼•å¯¹é½ã€‚
    """
    print(f"\nå¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹...")
    
    if isinstance(y, pd.Series):
        y_df = y.to_frame()
    else:
        y_df = y
        
    label_col = y_df.columns[0]

    # 1. åŒæ­¥å‡†å¤‡ï¼šå¼ºåˆ¶åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾ä»¥ç¡®ä¿å¯¹é½
    df_combined = pd.concat([X, y_df], axis=1, join='inner')

    # 2. è¡Œçº§åˆ«æ¸…æ´—
    # 2.1 åˆ é™¤æ ‡ç­¾ç¼ºå¤±çš„è¡Œ (è™½ç„¶ load_data å·²å¤„ç†ï¼Œæ­¤å¤„ä½œä¸ºåŒé‡ä¿é™©)
    df_combined = df_combined.dropna(subset=[label_col])

    # 2.2 å¤„ç† Mordred é”™è¯¯ç  (å°†éæ•°å€¼è½¬ä¸º NaN)
    X_temp = df_combined.drop(columns=[label_col])
    y_temp = df_combined[label_col]
    X_temp = X_temp.apply(pd.to_numeric, errors='coerce')

    # 2.3 åˆ é™¤å®Œå…¨é‡å¤çš„æ ·æœ¬
    df_combined = pd.concat([X_temp, y_temp], axis=1).drop_duplicates()
    
    # 3. ç‰¹å¾çº§åˆ«æ¸…æ´—
    X_curr = df_combined.drop(columns=[label_col])
    y_curr = df_combined[label_col]

    # 3.1 ç¼ºå¤±ç‡è¿‡æ»¤
    missing_ratios = X_curr.isnull().mean()
    cols_to_keep = missing_ratios[missing_ratios <= missing_ratio_limit].index
    X_curr = X_curr[cols_to_keep]
    print(f"åˆ é™¤ç¼ºå¤±ç‡ > {missing_ratio_limit*100}% çš„ç‰¹å¾åç»´åº¦: {X_curr.shape}")

    # 3.2 æ–¹å·®è¿‡æ»¤
    vars_series = X_curr.var()
    cols_var = vars_series[vars_series > variance_threshold].index
    X_curr = X_curr[cols_var]
    print(f"åˆ é™¤æ–¹å·® <= {variance_threshold} çš„å¸¸é‡ç‰¹å¾åç»´åº¦: {X_curr.shape}")

    # 3.3 é«˜ç›¸å…³æ€§è¿‡æ»¤
    if correlation_threshold and correlation_threshold < 1.0:
        corr_matrix = X_curr.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]
        X_curr = X_curr.drop(columns=to_drop)
        print(f"åˆ é™¤ç›¸å…³æ€§ > {correlation_threshold} çš„é«˜åº¦ç›¸å…³ç‰¹å¾åç»´åº¦: {X_curr.shape}")

    # 4. ç¼ºå¤±å€¼å¡«å…… (ä¸­ä½æ•°)
    imputer = SimpleImputer(strategy='median')
    X_imputed_val = imputer.fit_transform(X_curr)
    X_imputed = pd.DataFrame(X_imputed_val, columns=X_curr.columns, index=X_curr.index)

    # 5. ç‰¹å¾ç¼©æ”¾ (æ ‡å‡†åŒ–)
    scaler = StandardScaler()
    X_scaled_val = scaler.fit_transform(X_imputed)
    X_scaled = pd.DataFrame(X_scaled_val, columns=X_imputed.columns, index=y_curr.index)

    print(f"âœ… æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–å®Œæˆã€‚æœ€ç»ˆç‰¹å¾ç»´åº¦: {X_scaled.shape}")
    return X_scaled, y_curr

def auto_pca_reduction(df_scaled, target_variance=0.90, verbose=True):
    """
    è‡ªåŠ¨å¯»æ‰¾æœ€å°ä¸»æˆåˆ†æ•°é‡ä»¥æ»¡è¶³ç›®æ ‡ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”ä¾‹ï¼Œå¹¶è¿›è¡Œ PCA é™ç»´ã€‚
    """
    pca_full = PCA()
    pca_full.fit(df_scaled)
    cum_variance = np.cumsum(pca_full.explained_variance_ratio_)
    best_n = np.argmax(cum_variance >= target_variance) + 1

    if verbose:
        print(f"\nğŸ“Š ç´¯è®¡æ–¹å·®åˆ†æ:")
        print(f"   - è§£é‡Š {target_variance*100}% çš„æ–¹å·®ï¼Œéœ€è¦å‰ {best_n} ä¸ªä¸»æˆåˆ†ã€‚")
        print(f"   - ç»´åº¦å‹ç¼©ç‡: {(1 - best_n/df_scaled.shape[1])*100:.2f}%")

    pca_final = PCA(n_components=best_n)
    data_pca = pca_final.fit_transform(df_scaled)
    
    column_names = [f'PC{i+1}' for i in range(best_n)]
    df_pca = pd.DataFrame(data_pca, columns=column_names, index=df_scaled.index)

    return df_pca, pca_final
