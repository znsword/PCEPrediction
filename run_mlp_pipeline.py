import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import os

from code.config import (
    DATA_PATH, TARGET_COLUMN, CLEANING_PARAMS, 
    PCA_TARGET_VARIANCE, DEFAULT_MODEL_PARAMS, 
    RANDOM_STATE, TEST_SIZE
)
from code.data_utils import load_data, clean_data_manual, auto_pca_reduction
from code.models import MLPRegressor
from code.training import train_mlp, evaluate_mlp
from code.visualization import plot_predicted_vs_true

def run_pipeline():
    """
    è¿è¡Œå®Œæ•´çš„ MLP é¢„æµ‹å·¥ä½œæµã€‚
    """
    # 1. æ•°æ®åŠ è½½
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {DATA_PATH}")
        return

    X, y = load_data(DATA_PATH, target_column=TARGET_COLUMN)

    # 2. æ•°æ®æ¸…æ´— (åŒ…å«æ ‡å‡†åŒ–)
    X_clean, y_clean = clean_data_manual(X, y, **CLEANING_PARAMS)

    # 3. PCA é™ç»´
    X_reduced, pca_model = auto_pca_reduction(X_clean, target_variance=PCA_TARGET_VARIANCE)

    # 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_reduced, y_clean, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    # 5. è½¬æ¢ä¸º PyTorch å¼ é‡
    # æ³¨æ„ï¼šä½¿ç”¨ .values ç¡®ä¿å»æ‰ç´¢å¼•ï¼Œåªå–æ•°å€¼
    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

    # 6. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X_train_tensor, y_train_tensor)
    dataloader = DataLoader(dataset, batch_size=DEFAULT_MODEL_PARAMS['batch_size'], shuffle=True)

    # 7. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    input_dim = X_reduced.shape[1]
    model = MLPRegressor(
        input_dim=input_dim, 
        hidden_dim=DEFAULT_MODEL_PARAMS['hidden_dim'], 
        output_dim=DEFAULT_MODEL_PARAMS['output_dim']
    )
    
    optimizer = optim.Adam(model.parameters(), lr=DEFAULT_MODEL_PARAMS['learning_rate'])
    criterion = nn.MSELoss()

    # 8. æ¨¡å‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ MLP æ¨¡å‹...")
    train_mlp(
        model, 
        dataloader, 
        criterion, 
        optimizer, 
        num_epochs=DEFAULT_MODEL_PARAMS['num_epochs'], 
        verbose=True
    )

    # 9. æ¨¡å‹è¯„ä¼°
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°...")
    mae, rmse, r2 = evaluate_mlp(model, X_test_tensor, y_test_tensor)
    print(f"ğŸ”¹ æµ‹è¯•é›†æŒ‡æ ‡:")
    print(f"   - MAE: {mae:.4f}")
    print(f"   - RMSE: {rmse:.4f}")
    print(f"   - RÂ² Score: {r2:.4f}")

    # 10. ç»“æœå¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    with torch.no_grad():
        y_pred_tensor = model(X_test_tensor)
    
    plot_predicted_vs_true(
        y_test_tensor.cpu().numpy().flatten(), 
        y_pred_tensor.cpu().numpy().flatten(),
        title='MLP Results: Predicted vs. True PCE'
    )

if __name__ == "__main__":
    run_pipeline()
